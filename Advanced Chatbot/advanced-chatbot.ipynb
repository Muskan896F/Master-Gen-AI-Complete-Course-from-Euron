{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to build an advanced Chatbot with session memory using LangChain\n",
    "* Advanced Chatbot LLM App.\n",
    "    * Will be able to have a conversation.\n",
    "    * Will remember previous interactions: will have memory.\n",
    "    * Will be able to have different memories for different user sessions.\n",
    "    * Will be able to remember a limited number of messages: limited memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts included\n",
    "* Chat Model vs. LLM Model:\n",
    "    *  Chat Model is based around messages.\n",
    "    *  LLM Model is based around raw text.\n",
    "* Chat History: allows Chat Model to remember previous interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trick to avoid the nasty deprecation warnings from LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will use the LangChain legacy chain LLMChain. It works well, but LangChain displays a nasty deprecation warning. To avoid it, we will enter the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from langchain._api import LangChainDeprecationWarning\n",
    "\n",
    "warnings.simplefilter(\"ignore\", category=LangChainDeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect with the .env file located in the same directory of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect with an LLM and start a conversation with it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For this project, we will use OpenAI's gpt-3.5-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chatbot = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Human Message: the user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "messagesToTheChatbot = [\n",
    "    HumanMessage(content=\"My favourite color is blue.\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call the ChatModel (the LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Blue is often associated with calmness, serenity, and peace. It has a soothing effect and is often used in relaxing environments such as spas or bedrooms. Blue is also a versatile color that can be used in a variety of different shades, from light sky blue to deep navy blue. It is a timeless and classic color that is loved by many people.', response_metadata={'token_usage': {'completion_tokens': 73, 'prompt_tokens': 13, 'total_tokens': 86, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-4932c929-54ae-4837-b19d-5b457b7dedd6-0', usage_metadata={'input_tokens': 13, 'output_tokens': 73, 'total_tokens': 86})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot.invoke(messagesToTheChatbot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Track the operation in LangSmith\n",
    "* [Open LangSmith here](smith.langchain.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if the Chatbot remembers your favorite color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm sorry, I do not have enough information to determine your favorite color. Can you please provide me with more context or details?\", response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 13, 'total_tokens': 41, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-260d0b84-dd6d-4d07-bc9d-8a60bf030979-0', usage_metadata={'input_tokens': 13, 'output_tokens': 28, 'total_tokens': 41})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot.invoke([\n",
    "    HumanMessage(content=\"What is my favorite color?\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As you can see, our Chatbot cannot remember our previous interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's add memory to our Chatbot\n",
    "* We will use the ChatMessageHistory package.\n",
    "* We will save the Chatbot memory in a python dictionary called chatbotMemory.\n",
    "* We will define the get_session_history function to create a session_id for each conversation.\n",
    "* We will use the built-in runnable RunnableWithMesageHistory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "\n",
    "\n",
    "chatbotMemory = {}\n",
    "\n",
    "# input: session_id, output: chatbotMemory[session_id]\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in chatbotMemory:\n",
    "        chatbotMemory[session_id] = ChatMessageHistory()\n",
    "    return chatbotMemory[session_id]\n",
    "\n",
    "\n",
    "chatbot_with_message_history = RunnableWithMessageHistory(\n",
    "    chatbot, \n",
    "    get_session_history\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is BaseChatMessageHistory and what it does?\n",
    "BaseChatMessageHistory is what is called an **abstract base class** in Python. [See more info about this here](https://api.python.langchain.com/en/latest/chat_history/langchain_core.chat_history.BaseChatMessageHistory.html). This means it serves as a template or a foundational **blueprint for other classes**. It outlines a set of methods and structures that any class inheriting from it must implement or adhere to, but it cannot be used to create objects directly.\n",
    "\n",
    "Here's a simpler breakdown of what it means for `BaseChatMessageHistory` to be an abstract base class:\n",
    "\n",
    "1. **Blueprint for Other Classes:** It provides a predefined structure that other classes can follow. Think of it like an outline or a checklist for building something; it specifies what needs to be included, but it isnâ€™t the final product.\n",
    "\n",
    "2. **Cannot Create Instances:** You cannot create an instance of an abstract base class. Trying to create an object directly from `BaseChatMessageHistory` would result in an error because it's meant to be a guide, not something to use directly.\n",
    "\n",
    "3. **Requires Implementation:** Any class that inherits from this abstract base class needs to implement specific methods outlined in `BaseChatMessageHistory`, such as methods for adding messages, retrieving messages, and clearing messages. The class sets the rules, and the subclasses need to follow these rules by providing the actual operational details.\n",
    "\n",
    "4. **Purpose in Design:** Using an abstract base class helps ensure consistency and correctness in the implementation of classes that extend it. It's a way to enforce certain functionalities in any subclass, making sure that they all behave as expected without rewriting the same code multiple times.\n",
    "\n",
    "Overall, the concept of an abstract base class is about setting standards and rules, while leaving the specific details of execution to be defined by subclasses that inherit from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's explain the previous code in simple terms\n",
    "The previous code manages the chatbot's memory of conversations based on session identifiers. Hereâ€™s a breakdown of what the different components do:\n",
    "\n",
    "1. **chatbotMemory**:\n",
    "    - `chatbotMemory = {}`: This initializes an empty dictionary where session IDs and their respective chat histories will be stored.\n",
    "\n",
    "2. **get_session_history Function**:\n",
    "    - This function, `get_session_history`, takes a `session_id` as an argument and returns the chat history associated with that session.\n",
    "    - If a chat history for the given `session_id` does not exist in `chatbotMemory`, a new instance of `ChatMessageHistory` is created and assigned to that `session_id` in the dictionary.\n",
    "    - The function ensures that each session has its own unique chat history, stored and retrieved using the session ID.\n",
    "\n",
    "3. **chatbot_with_message_history**:\n",
    "    - `chatbot_with_message_history = RunnableWithMessageHistory(chatbot, get_session_history)`: This line creates an instance of `RunnableWithMessageHistory` using two arguments: `chatbot` and `get_session_history`.\n",
    "    - The `chatbot` is passed along with the `get_session_history` function. This setup integrates the chatbot with the functionality to handle session-specific chat histories, allowing the chatbot to maintain continuity and context in conversations across different sessions.\n",
    "    - **Learn more about RunnableWithMessageHistory** [here](https://python.langchain.com/v0.1/docs/expression_language/how_to/message_history/).\n",
    "\n",
    "Overall, the code organizes and manages a chatbot's memory, enabling it to handle different sessions with users effectively by remembering previous messages within each session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RunnableWithMessageHistory\n",
    "**When invoking a new RunnableWithMessageHistory, we specify the corresponding chat history using a configurable parameter**. Let's say we want to create a chat memory for one user session, let's call it session1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "session1 = {\"configurable\": {\"session_id\": \"001\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Red is such a bold and vibrant color! It can symbolize passion, energy, and strength. What do you love most about the color red?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responseFromChatbot = chatbot_with_message_history.invoke(\n",
    "    [HumanMessage(content=\"My favorite color is red.\")],\n",
    "    config=session1,\n",
    ")\n",
    "\n",
    "responseFromChatbot.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your favorite color is red!'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responseFromChatbot = chatbot_with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my favorite color?\")],\n",
    "    config=session1,\n",
    ")\n",
    "\n",
    "responseFromChatbot.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's now change the session_id and see what happens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a chat memory for another user session, let's call it session2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "session2 = {\"configurable\": {\"session_id\": \"002\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the chatbot is using this new memory for session2, it will not be able to remember anything from the previous conversation in the session1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I cannot determine your favorite color as I do not have enough information about you. Can you please provide me with more details or preferences so I can help you figure out your favorite color?\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responseFromChatbot = chatbot_with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my favorite color?\")],\n",
    "    config=session2,\n",
    ")\n",
    "\n",
    "responseFromChatbot.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's go back to session1 and see if the memory is still there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "session1 = {\"configurable\": {\"session_id\": \"001\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sorry about that mistake earlier! I don't actually know what your favorite color is. Could you please tell me?\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responseFromChatbot = chatbot_with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my favorite color?\")],\n",
    "    config=session1,\n",
    ")\n",
    "\n",
    "responseFromChatbot.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the chatbot is now able to remember the session1 conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our ChatBot has session memory now. Let's check if it remembers the conversation from session2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "session2 = {\"configurable\": {\"session_id\": \"002\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nice to meet you, Prince! What is your favorite color?'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "responseFromChatbot = chatbot_with_message_history.invoke(\n",
    "    [HumanMessage(content=\"My name is prince.\")],\n",
    "    config=session2,\n",
    ")\n",
    "\n",
    "responseFromChatbot.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Prince.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responseFromChatbot = chatbot_with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What is my name?\")],\n",
    "    config=session2,\n",
    ")\n",
    "\n",
    "responseFromChatbot.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I don't have the capability to know personal details about users unless they have shared that information with me in the course of our conversation. If you'd like to share your favorite color with me, I'd be happy to know!\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responseFromChatbot = chatbot_with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What is my favorite color?\")],\n",
    "    config=session1,\n",
    ")\n",
    "\n",
    "responseFromChatbot.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our chatBot now remembers each of our conversations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The importance to manage the Conversation History\n",
    "* The memory of a chatbot is included in the context window of the LLM so, if left unmanaged, can potentially overflow it.\n",
    "* **We are now going to learn how to limit the size of the memory of a chatbot**.\n",
    "* First, let's take a look at what is in the memory of our chatbot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'001': InMemoryChatMessageHistory(messages=[HumanMessage(content='My favorite color is red.'), AIMessage(content='Red is such a bold and vibrant color! It can symbolize passion, energy, and strength. What do you love most about the color red?', response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 13, 'total_tokens': 44, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-3ad820d6-0cfd-4c09-a1cb-cae0ab35cffe-0', usage_metadata={'input_tokens': 13, 'output_tokens': 31, 'total_tokens': 44}), HumanMessage(content=\"What's my favorite color?\"), AIMessage(content='Your favorite color is red!', response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 57, 'total_tokens': 64, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-c56c27c1-e197-41e7-a939-7994c0779378-0', usage_metadata={'input_tokens': 57, 'output_tokens': 7, 'total_tokens': 64}), HumanMessage(content=\"What's my favorite color?\"), AIMessage(content=\"Sorry about that mistake earlier! I don't actually know what your favorite color is. Could you please tell me?\", response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 77, 'total_tokens': 101, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-f6913eaa-319a-4f0e-828f-8baad91cab19-0', usage_metadata={'input_tokens': 77, 'output_tokens': 24, 'total_tokens': 101}), HumanMessage(content='What is my favorite color?'), AIMessage(content=\"I'm sorry, I don't have the capability to know personal details about users unless they have shared that information with me in the course of our conversation. If you'd like to share your favorite color with me, I'd be happy to know!\", response_metadata={'token_usage': {'completion_tokens': 51, 'prompt_tokens': 114, 'total_tokens': 165, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-506bf805-e070-4991-bfbd-f812c39f28b6-0', usage_metadata={'input_tokens': 114, 'output_tokens': 51, 'total_tokens': 165})]), '002': InMemoryChatMessageHistory(messages=[HumanMessage(content=\"What's my favorite color?\"), AIMessage(content=\"I'm sorry, I cannot determine your favorite color as I do not have enough information about you. Can you please provide me with more details or preferences so I can help you figure out your favorite color?\", response_metadata={'token_usage': {'completion_tokens': 42, 'prompt_tokens': 13, 'total_tokens': 55, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-86e34cd0-b865-46a0-8f54-1f393d5b03b7-0', usage_metadata={'input_tokens': 13, 'output_tokens': 42, 'total_tokens': 55}), HumanMessage(content='My name is prince.'), AIMessage(content='Nice to meet you, Prince! What is your favorite color?', response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 67, 'total_tokens': 81, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-f430149e-075b-42cb-8e2f-3e8c0eabb892-0', usage_metadata={'input_tokens': 67, 'output_tokens': 14, 'total_tokens': 81}), HumanMessage(content='What is my name?'), AIMessage(content='Your name is Prince.', response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 93, 'total_tokens': 99, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-234928a9-82f4-4b33-a194-9e7faacddbec-0', usage_metadata={'input_tokens': 93, 'output_tokens': 6, 'total_tokens': 99})])}\n"
     ]
    }
   ],
   "source": [
    "print(chatbotMemory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now, **let's define a function to limit the number of messages stored in memory and add it to our chain with .assign**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "def limited_memory_of_messages(messages, number_of_messages_to_keep=2):\n",
    "    return messages[-number_of_messages_to_keep:]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "limitedMemoryChain = (\n",
    "    RunnablePassthrough.assign(messages=lambda x: limited_memory_of_messages(x[\"messages\"]))\n",
    "    | prompt \n",
    "    | chatbot\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The limited_memory_of_messages function allows you to trim the list of stored messages, keeping only a specified number of the latest ones. For example, if you have a long list of messages and you only want to keep the last two, this function will do that for you.\n",
    "* The lambda function works in conjunction with the `limited_memory_of_messages` function. Hereâ€™s a simple breakdown:\n",
    "\n",
    "    1. **Lambda Function**: The `lambda` keyword is used to create a small anonymous function in Python. The `lambda` function defined here takes one argument, `x`.\n",
    "\n",
    "    2. **Function Argument**: The argument `x` is expected to be a dictionary that contains a key named `\"messages\"`. The value associated with this key is a list of messages.\n",
    "\n",
    "    3. **Function Body**: The body of the `lambda` function calls the `limited_memory_of_messages` function. It passes the list of messages found in `x[\"messages\"]` to this function.\n",
    "\n",
    "    4. **Default Behavior of limited_memory_of_messages**: Since the `lambda` function does not specify the `number_of_messages_to_keep` parameter when it calls `limited_memory_of_messages`, the latter will default to keeping the last 2 messages from the list (as defined by the earlier function).\n",
    "\n",
    "In essence, the `lambda` function is a shorthand way to apply the `limited_memory_of_messages` function to the message list contained within a dictionary. It automatically trims the list to the last two messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's now create our new chatbot with limited message history**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_with_limited_message_history = RunnableWithMessageHistory(\n",
    "    limitedMemoryChain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's add 2 more messages to the session1 conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vespa scooters are iconic and stylish! They have a classic design that never goes out of style. Do you own a Vespa scooter, or are you a fan of them in general?'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responseFromChatbot = chatbot_with_message_history.invoke(\n",
    "    [HumanMessage(content=\"My favorite vehicles are Vespa scooters.\")],\n",
    "    config=session1,\n",
    ")\n",
    "\n",
    "responseFromChatbot.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'San Francisco is a beautiful city known for its iconic landmarks such as the Golden Gate Bridge, Alcatraz Island, and the cable cars. The city has a rich history, diverse culture, and stunning views. What do you love most about San Francisco?'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responseFromChatbot = chatbot_with_message_history.invoke(\n",
    "    [HumanMessage(content=\"My favorite city is San Francisco.\")],\n",
    "    config=session1,\n",
    ")\n",
    "\n",
    "responseFromChatbot.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The chatbot memory has now 4 messages. Let's check the Chatbot with limited memory. \n",
    "* Remember, this chatbot only remembers the last 2 messages, so if we ask her about the first message she should not remember it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm glad to hear that San Francisco is your favorite city! There are so many things to love about it, from the picturesque views to the vibrant culture. Is there anything specific you enjoy the most about San Francisco?\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responseFromChatbot = chatbot_with_limited_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"what My favorite city?\")],\n",
    "    },\n",
    "    config=session1,\n",
    ")\n",
    "\n",
    "responseFromChatbot.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The chatbot with limited memory has behaved as we expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, let's compare the previous response with the one provided by the Chatbot with unlimited memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your favorite color is red!'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responseFromChatbot = chatbot_with_message_history.invoke(\n",
    "    [HumanMessage(content=\"what is my favorite color?\")],\n",
    "    config=session1,\n",
    ")\n",
    "\n",
    "responseFromChatbot.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As you can see, this chatbot remembers our first message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to execute the code from Visual Studio Code\n",
    "* In Visual Studio Code, see the file 004-invoke-stream-batch.py\n",
    "* In terminal, make sure you are in the directory of the file and run:\n",
    "    * python 002-advanced-chatbot.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The End\n"
     ]
    }
   ],
   "source": [
    "print(\"The End\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
